import torch
import tiktoken
from gpt2 import GPT, device
import time
import math
import os
from tqdm import tqdm
import logging
import datetime
import traceback

def train():
    """Training Process Implementation"""
    # Create necessary directories
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('logs', exist_ok=True)

    # Configure logging with consistent format
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'logs/training_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
            logging.StreamHandler()
        ]
    )

    # Initial welcome banner
    logging.info("\n" + "="*100)
    logging.info("    üöÄ GPT-2 Training Process - An Educational Journey")
    logging.info("    This interactive guide will walk you through every step of training")
    logging.info("    a GPT-2 language model, explaining what's happening in real-time!")
    logging.info("="*100)

    # Safe device detection
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    # Data loading with detailed statistics
    logging.info("\nüìö Loading Training Data\n------------------------\nLet's look at what we're working with...")

    try:
        with open("data.txt", "r", encoding='utf-8') as f:
            text = f.read()

        # Split long log messages into multiple calls
        logging.info("\nüìä Dataset Statistics\n-------------------")
        logging.info("üìù Raw Text:")
        logging.info(f"   ‚Ä¢ Total Characters: {len(text):,}")
        logging.info(f"   ‚Ä¢ Unique Characters: {len(set(text)):,}")
        logging.info(f"   ‚Ä¢ Lines of Text: {text.count(chr(10)):,}")
        logging.info(f"   ‚Ä¢ Average Line Length: {len(text)/max(1, text.count(chr(10))):.1f} chars")
        logging.info(f"\nüî§ Sample Content:\n   \"{text[:100]}...\"")

        # Tokenization with explanation
        tokenizer = tiktoken.get_encoding("gpt2")
        tokens = tokenizer.encode(text)
        sample_tokens = tokens[:10]
        sample_words = tokenizer.decode(sample_tokens)

        logging.info("\nüéØ Tokenization Results\n---------------------")
        logging.info("üìä Token Statistics:")
        logging.info(f"   ‚Ä¢ Total Tokens: {len(tokens):,}")
        logging.info(f"   ‚Ä¢ Vocabulary Coverage: {len(set(tokens)):,} unique tokens")
        logging.info(f"   ‚Ä¢ Compression Ratio: {len(tokens)/len(text):.2f} tokens per character")
        logging.info(f"\nüîç Example Tokenization:")
        logging.info(f"   Text: \"{sample_words}\"")
        logging.info(f"   Tokens: {sample_tokens}")

        # Convert to tensor and split data
        tokens = torch.tensor(tokens, dtype=torch.long, device=device)
        n_train = int(0.9 * len(tokens))
        train_data = tokens[:n_train]
        val_data = tokens[n_train:]

        # Model initialization with architecture breakdown
        logging.info("""
ü§ñ Initializing Neural Network
---------------------------
Building a GPT-2 model with the following architecture:""")

        model = GPT().to(device)

        logging.info(f"""
üìê Model Architecture
------------------
üî∏ Input Processing:
   ‚Ä¢ Vocabulary Size: {model.vocab_size:,} tokens
   ‚Ä¢ Embedding Dimension: {model.d_model} units
   ‚Ä¢ Maximum Sequence Length: {model.block_size} tokens

üî∏ Transformer Blocks:
   ‚Ä¢ Number of Layers: {len(model.blocks)}
   ‚Ä¢ Attention Heads per Layer: {model.n_heads}
   ‚Ä¢ MLP Dimension: 3072 units

üî∏ Total Parameters: {sum(p.numel() for p in model.parameters()):,}
""")

        # Initialize optimizer and learning rate scheduler
        batch_size = 16
        sequence_length = 512
        learning_rate = 1e-3
        epochs = 5

        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=2, verbose=True
        )

        # Training configuration with explanations
        logging.info("""
‚öôÔ∏è Training Configuration
----------------------
Setting up the learning process with carefully chosen parameters:""")

        logging.info(f"""
üìà Hyperparameters:
   ‚Ä¢ Batch Size: {batch_size} sequences
     ‚Üí Processes {batch_size} text chunks at once for stable learning

   ‚Ä¢ Sequence Length: {sequence_length} tokens
     ‚Üí Each chunk is {sequence_length} tokens long for context

   ‚Ä¢ Learning Rate: {learning_rate}
     ‚Üí Controls how fast the model updates its understanding

   ‚Ä¢ Training Epochs: {epochs}
     ‚Üí Will see the entire dataset {epochs} times

üîß Optimizer: AdamW
   ‚Üí Advanced optimization algorithm that adapts the learning rate
   ‚Üí Includes weight decay for better generalization

üìâ Learning Rate Scheduler:
   ‚Üí Reduces learning rate when progress plateaus
   ‚Üí Helps fine-tune the model's understanding
""")

        # Training loop with rich progress information
        best_val_loss = float('inf')
        start_time = time.time()

        for epoch in range(epochs):
            epoch_start = time.time()
            logging.info(f"""
üéØ Starting Epoch {epoch+1}/{epochs}
{'='*50}""")

            model.train()
            total_loss = 0
            num_batches = len(train_data) // (batch_size * sequence_length)
            progress_bar = tqdm(range(num_batches), desc=f"Epoch {epoch+1}/{epochs}")

            for batch_num in progress_bar:
                batch_start = time.time()

                # Get batch
                start_idx = batch_num * batch_size * sequence_length
                x = train_data[start_idx:start_idx + batch_size * sequence_length].view(batch_size, sequence_length)
                y = train_data[start_idx+1:start_idx + batch_size * sequence_length + 1].view(batch_size, sequence_length)

                # Forward pass
                logits, loss = model(x, y)

                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

                # Update metrics
                total_loss += loss.item()
                avg_loss = total_loss / (batch_num + 1)
                batch_time = time.time() - batch_start
                tokens_per_sec = batch_size * sequence_length / batch_time

                if batch_num % 100 == 0:
                    # Convert tensor to list for tokenizer
                    sample_text = tokenizer.decode(x[0].cpu().tolist()[:50])

                    logging.info("\nüìä Training Progress - Batch {batch_num}/{num_batches}")
                    logging.info("---------------------------------")
                    logging.info("üî∏ Loss Metrics:")
                    logging.info(f"   ‚Ä¢ Current Batch Loss: {loss.item():.4f}")
                    logging.info(f"   ‚Ä¢ Average Loss: {avg_loss:.4f}")
                    logging.info(f"   ‚Ä¢ Best Loss So Far: {best_val_loss:.4f}")
                    logging.info("\n‚ö° Performance:")
                    logging.info(f"   ‚Ä¢ Processing Speed: {tokens_per_sec:.0f} tokens/second")
                    logging.info(f"   ‚Ä¢ Time per Batch: {batch_time:.2f}s")

                    # Safe GPU memory logging
                    if torch.cuda.is_available():
                        gpu_memory = torch.cuda.max_memory_allocated()/1e9
                        logging.info(f"   ‚Ä¢ GPU Memory: {gpu_memory:.1f}GB")

                    logging.info(f"\nüéØ Sample Input:\n   \"{sample_text}...\"")
                    logging.info("\nüí´ Learning Progress:")
                    logging.info(f"   ‚Ä¢ Completed: {batch_num/num_batches*100:.1f}% of epoch")
                    logging.info(f"   ‚Ä¢ Elapsed Time: {time.time() - epoch_start:.1f}s")
                    logging.info(f"   ‚Ä¢ Estimated Time Remaining: {(time.time() - epoch_start) * (num_batches-batch_num) / (batch_num+1):.1f}s")

            # Epoch summary
            epoch_time = time.time() - epoch_start
            logging.info(f"""
üåü Epoch {epoch+1} Complete!
-------------------------
üìä Performance Metrics:
   ‚Ä¢ Final Loss: {avg_loss:.4f}
   ‚Ä¢ Time Taken: {epoch_time:.1f}s
   ‚Ä¢ Average Speed: {len(train_data)/epoch_time:.0f} tokens/second

üíæ Saving Checkpoint...
""")

            # Save checkpoint
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'config': {
                    'batch_size': batch_size,
                    'sequence_length': sequence_length,
                    'vocab_size': model.vocab_size,
                    'd_model': model.d_model,
                    'n_heads': model.n_heads,
                }
            }, f'checkpoints/model_epoch_{epoch+1}.pt')

    except Exception as e:
        logging.error("\n‚ùå Error Encountered")
        logging.error("-----------------")
        logging.error(str(e))
        logging.error("\nüí° Troubleshooting Tips:")
        logging.error("‚Ä¢ Check if data.txt exists and is readable")

        # Safe GPU memory checking
        if torch.cuda.is_available():
            try:
                gpu_memory = torch.cuda.get_device_properties(0).total_memory/1e9
                logging.error(f"‚Ä¢ GPU Memory Available: {gpu_memory:.1f}GB")
            except Exception:
                logging.error("‚Ä¢ GPU detected but unable to query memory")
        else:
            logging.error("‚Ä¢ Running on CPU - consider using GPU for faster training")

        logging.error("‚Ä¢ Ensure all required packages are installed")
        logging.error("‚Ä¢ Try reducing batch size or sequence length if out of memory")
        logging.error("\nüîç Error Details:")
        logging.error(traceback.format_exc())
        raise

def evaluate_model(model, tokens, batch_size=16, sequence_length=512):
    """Evaluate model on validation data"""
    model.eval()
    total_loss = 0
    num_batches = len(tokens) // (batch_size * sequence_length)

    with torch.no_grad():
        for i in range(num_batches):
            # Get batch
            start_idx = i * batch_size * sequence_length
            end_idx = start_idx + batch_size * sequence_length
            x = tokens[start_idx:end_idx].view(batch_size, sequence_length)
            y = tokens[start_idx+1:end_idx+1].view(batch_size, sequence_length)

            # Forward pass
            logits, loss = model(x, y)
            total_loss += loss.item()

    return total_loss / num_batches

def configure_optimizer(model):
    """
    Adam Optimizer Configuration (MATH.md Section 10.1)

    Update Rules:
    m‚Çú = Œ≤‚ÇÅm‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)g‚Çú (momentum)
    v‚Çú = Œ≤‚ÇÇv‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÇ)g‚Çú¬≤ (velocity)

    Parameter Update:
    Œ∏‚Çú = Œ∏‚Çú‚Çã‚ÇÅ - Œ∑ * mÃÇ‚Çú/‚àö(vÃÇ‚Çú + Œµ)

    Hyperparameters:
    - Learning rate (Œ∑): 1e-3
    - Beta1 (Œ≤‚ÇÅ): 0.9 (momentum decay)
    - Beta2 (Œ≤‚ÇÇ): 0.999 (velocity decay)
    - Epsilon (Œµ): 1e-8 (numerical stability)
    """

if __name__ == "__main__":
    train()
